{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import langchain\n",
    "from langchain_community import document_loaders\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    SystemMessagePromptTemplate\n",
    ")\n",
    "\n",
    "from langchain.schema import HumanMessage, SystemMessage\n",
    "from langchain_openai import OpenAI, OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.chains import RetrievalQA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import dotenv\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFLoader(\"/home/biniyam/Prompt-Generation/data/1706.03762.pdf\")\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=250)\n",
    "doc = text_splitter.split_documents(loader.load_and_split())\n",
    "vectorstore = Chroma.from_documents(doc, OpenAIEmbeddings())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "persist_directory = 'db'\n",
    "embedding = OpenAIEmbeddings()\n",
    "vectordb = Chroma.from_documents(documents=doc,\n",
    "                                embedding=embedding,\n",
    "                                persist_directory=persist_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordb.persist()\n",
    "vectordb = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordb = Chroma(persist_directory=persist_directory, \n",
    "                  embedding_function=embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectordb.as_retriever(search_kwargs={'k': 3})\n",
    "documents = retriever.get_relevant_documents(\"what is the paper talking about?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a chain to answer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans_chain = RetrievalQA.from_chain_type(llm=OpenAI(),\n",
    "                                        chain_type='stuff',\n",
    "                                        retriever=retriever,\n",
    "                                        return_source_documents=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_response(llm_response):\n",
    "    print(llm_response['result'])\n",
    "    # print(\"\\n\\nSources\")\n",
    "    # for source in llm_response[\"source_documents\"]:\n",
    "    #     print(source.metadata['source'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Some possible prompts could be:\n",
      "- Can you explain the concept of \"input and output modalities\" and how they relate to the Transformer model?\n",
      "- How do local, restricted attention mechanisms differ from traditional attention mechanisms in handling large inputs and outputs?\n",
      "- Can you provide an example of a task where the Transformer has been used successfully?\n",
      "- How does the Transformer model handle tasks such as reading comprehension and summarization?\n",
      "- Can you explain the significance of task-independent sentence representations and how they are learned in the Transformer model?\n"
     ]
    }
   ],
   "source": [
    "query = \"can you generate a prompt that i am going to use in another llm session that will help me better understand the paper. your job here is to generate a list of prompts that i will use to query an llm with, since human language is so simple and might not get me better results.\"\n",
    "llm_response = ans_chain(query)\n",
    "process_response(llm_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='1. \"What are the key features and benefits of using Pydantic in Python?\"\\n2. \"Can you provide a step-by-step guide on how to install and set up Pydantic in a Python project?\"\\n3. \"What are some best practices for using Pydantic effectively in Python?\"\\n4. \"How does Pydantic handle data validation and type checking in Python?\"\\n5. \"Can you give examples of how to define and use Pydantic models in Python?\"\\n6. \"What are some advanced techniques or use cases for leveraging Pydantic in Python?\"\\n7. \"Are there any performance considerations or optimizations to keep in mind when using Pydantic in Python?\"\\n8. \"How does Pydantic integrate with other Python libraries or frameworks, such as FastAPI or Django?\"\\n9. \"What are some common pitfalls or challenges when working with Pydantic in Python, and how can they be overcome?\"\\n10. \"Can you provide a comparison between Pydantic and other similar libraries in Python, such as dataclasses or marshmallow?\"')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# messages = [\n",
    "#     SystemMessage(\n",
    "#         content=\"You are a very helpful assistant that generates multiple prompts that the user copies and uses to get better results by prompting llms by taking in user intent and context as input and then provide the user with multiple prompt candidates. For example if i have a simple prompt, your job is to give the me a superhuman version of the original prompt that can give me the best results when use that prompt against you or any other llm. The prompts you give are superior and more mature and are prompts that can generate as better output than my original prompt. \"\n",
    "#     ),\n",
    "\n",
    "#     HumanMessage(\n",
    "#         content=\"How do i use pydantic in python\"\n",
    "#     )\n",
    "# ]\n",
    "# chat(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ten",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
