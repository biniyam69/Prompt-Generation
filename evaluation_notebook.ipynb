{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def monte_carlo_eval(prompt):\n",
    "    # Simulating different types of responses\n",
    "    response_types = ['highly relevant', 'somewhat relevant', 'irrelevant']\n",
    "    scores = {'highly relevant': 3, 'somewhat relevant': 2, 'irrelevant': 1}\n",
    "\n",
    "    # Perform multiple random trials\n",
    "    trials = 100\n",
    "    total_score = 0\n",
    "    for _ in range(trials):\n",
    "        response = random.choice(response_types)\n",
    "        total_score += scores[response]\n",
    "\n",
    "    # Average score represents the evaluation\n",
    "    return total_score / trials\n",
    "\n",
    "def elo_eval(prompt, base_rating=1500):\n",
    "    # Simulate the outcome of the prompt against standard criteria\n",
    "    # Here, we randomly decide if the prompt 'wins', 'loses', or 'draws'\n",
    "    outcomes = ['win', 'loss', 'draw']\n",
    "    outcome = random.choice(outcomes)\n",
    "\n",
    "    # Elo rating formula parameters\n",
    "    K = 30  # Maximum change in rating\n",
    "    R_base = 10 ** (base_rating / 400)\n",
    "    R_opponent = 10 ** (1600 / 400)  # Assuming a fixed opponent rating\n",
    "    expected_score = R_base / (R_base + R_opponent)\n",
    "\n",
    "    # Calculate the new rating based on the outcome\n",
    "    actual_score = {'win': 1, 'loss': 0, 'draw': 0.5}[outcome]\n",
    "    new_rating = base_rating + K * (actual_score - expected_score)\n",
    "\n",
    "    return new_rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What project did OpenAI showcase in 2018?: 1590.8215462818882\n",
      "Who founded OpenAI?: 1551.2547216834469\n",
      "How did the AI agents in OpenAI Five work together?: 1545.5864698331568\n",
      "What was the initial goal of OpenAI?: 1541.5568025583048\n",
      "What did OpenAI release in 2016?: 1490.7235766925844\n"
     ]
    }
   ],
   "source": [
    "def elo_ratings_func(prompts, elo_ratings, K=30, opponent_rating=1600):\n",
    "    \"\"\"\n",
    "    Update Elo ratings for a list of prompts based on simulated outcomes.\n",
    "\n",
    "    Parameters:\n",
    "    prompts (list): List of prompts to be evaluated.\n",
    "    elo_ratings (dict): Current Elo ratings for each prompt.\n",
    "    K (int): Maximum change in rating.\n",
    "    opponent_rating (int): Fixed rating of the opponent for simulation.\n",
    "\n",
    "    Returns:\n",
    "    dict: Updated Elo ratings.\n",
    "    \"\"\"\n",
    "\n",
    "    for prompt in prompts:\n",
    "        # Simulate an outcome against the standard criteria or another prompt\n",
    "        outcome = random.choice(['win', 'loss', 'draw'])\n",
    "\n",
    "        # Calculate the new rating based on the outcome\n",
    "        actual_score = {'win': 1, 'loss': 0, 'draw': 0.5}[outcome]\n",
    "        R_base = 10 ** (elo_ratings[prompt] / 400)\n",
    "        R_opponent = 10 ** (opponent_rating / 400)\n",
    "        expected_score = R_base / (R_base + R_opponent)\n",
    "        elo_ratings[prompt] += K * (actual_score - expected_score)\n",
    "\n",
    "    return elo_ratings\n",
    "\n",
    "# Example usage\n",
    "prompts = [\"Who founded OpenAI?\", \n",
    "                \"What was the initial goal of OpenAI?\",\n",
    "                \"What did OpenAI release in 2016?\", \n",
    "                \"What project did OpenAI showcase in 2018?\",\n",
    "                \"How did the AI agents in OpenAI Five work together?\"\n",
    "                ]\n",
    "elo_ratings = {prompt: 1500 for prompt in prompts}  # Initial ratings\n",
    "\n",
    "# Conduct multiple rounds of evaluation\n",
    "for _ in range(10):  # Number of rounds\n",
    "    elo_ratings = elo_ratings_func(prompts, elo_ratings)\n",
    "\n",
    "# Sort prompts by their final Elo ratings\n",
    "sorted_prompts = sorted(prompts, key=lambda x: elo_ratings[x], reverse=True)\n",
    "\n",
    "# Print the ranked prompts\n",
    "for prompt in sorted_prompts:\n",
    "    print(f\"{prompt}: {elo_ratings[prompt]}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prompts evaluation\n",
    "#### \"What was the initial goal of OpenAI?\": 1583.6551603182484\n",
    "This prompt has the highest rating,  suggesting it was evaluated as the most relevant, accurate, or valuable.\n",
    "#### \"Who founded OpenAI?\": 1550.8315837034786\n",
    "This prompt also performed well, but slightly less so than the first one.\n",
    "#### \"What project did OpenAI showcase in 2018?\": 1524.894352475904 Moderate\n",
    "#### \"What did OpenAI release in 2016?\": 1518.8441077283887\n",
    "These prompts have lower ratings, indicating they were evaluated as less relevant or valuable compared to the top-rated prompts.\n",
    "#### \"How did the AI agents in OpenAI Five work together?\": 1501.4300442180024\n",
    "This prompt is closer to the baseline rating, suggesting its performance was near average in your evaluation criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_prompt(main_prompt, test_cases):\n",
    "    evaluations = {}\n",
    "\n",
    "    # Evaluate the main prompt using Monte Carlo and Elo methods\n",
    "    evaluations['main_prompt'] = {\n",
    "        'Monte Carlo Evaluation': monte_carlo_eval(main_prompt),\n",
    "        'Elo Rating Evaluation': elo_eval(main_prompt)\n",
    "    }\n",
    "\n",
    "    # Evaluate each test case\n",
    "    for idx, test_case in enumerate(test_cases):\n",
    "        evaluations[f'test_case_{idx+1}'] = {\n",
    "            'Monte Carlo Evaluation': monte_carlo_eval(test_case),\n",
    "            'Elo Rating Evaluation': elo_eval(test_case)\n",
    "        }\n",
    "\n",
    "    return evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'main_prompt': {'Monte Carlo Evaluation': 2.06, 'Elo Rating Evaluation': 1519.2019499940866}, 'test_case_1': {'Monte Carlo Evaluation': 2.09, 'Elo Rating Evaluation': 1519.2019499940866}, 'test_case_2': {'Monte Carlo Evaluation': 2.18, 'Elo Rating Evaluation': 1519.2019499940866}, 'test_case_3': {'Monte Carlo Evaluation': 2.04, 'Elo Rating Evaluation': 1504.2019499940866}, 'test_case_4': {'Monte Carlo Evaluation': 1.85, 'Elo Rating Evaluation': 1519.2019499940866}, 'test_case_5': {'Monte Carlo Evaluation': 2.05, 'Elo Rating Evaluation': 1489.2019499940866}}\n"
     ]
    }
   ],
   "source": [
    "main_prompt = \"why we use OepenAI?\"\n",
    "test_cases = [\"Who founded OpenAI?\", \n",
    "                \"What was the initial goal of OpenAI?\",\n",
    "                \"What did OpenAI release in 2016?\", \n",
    "                \"What project did OpenAI showcase in 2018?\",\n",
    "                \"How did the AI agents in OpenAI Five work together?\"\n",
    "                ]\n",
    "result = evaluate_prompt(main_prompt, test_cases)\n",
    "print(result)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interprtation\n",
    "#### 1. Monte Carlo Evaluation:\n",
    "Scores Range: From 1 to 3, with higher scores indicating greater relevance or quality of the prompt.\n",
    "###### Interpretation:\n",
    "1.94 (Main Prompt): Slightly below average relevance or quality.\n",
    "2.06, 2.02, 1.89, 1.98, 2.03 (Test Cases): Scores around 2 suggest moderate relevance or quality. The variation indicates some test cases are deemed slightly more relevant or higher quality than others.\n",
    "#### 2. Elo Rating Evaluation:\n",
    "Base Rating: Usually starts at 1500, with changes based on the 'performance' of the prompt against a set of standards.\n",
    "Higher than 1500: Indicates the prompt performed better than average.\n",
    "Lower than 1500: Indicates the prompt performed worse than average.\n",
    "###### Interpretation:\n",
    "1489.20 (Main Prompt): Slightly below the average performance.\n",
    "1519.20 (Test Cases 1, 2, 4, 5): These prompts are rated above the average, suggesting better performance.\n",
    "1504.20 (Test Case 3): Slightly above average performance.\n",
    "#### Overall Interpretation:\n",
    "Main Prompt: Both evaluations suggest that the main prompt is slightly below average in terms of relevance and quality.\n",
    "Test Cases: Generally, the test cases are rated as average or slightly above average in both relevance and quality. Test Cases 1, 2, 4, and 5 seem to perform particularly well in the Elo evaluation, indicating they might be more effective or well-structured prompts compared to the main prompt and Test Case 3."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAGAS Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter  \n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Weaviate\n",
    "import weaviate\n",
    "from weaviate.embedded import EmbeddedOptions\n",
    "from dotenv import load_dotenv,find_dotenv\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Weaviate\n",
    "import weaviate\n",
    "from weaviate.embedded import EmbeddedOptions\n",
    "from dotenv import load_dotenv,find_dotenv\n",
    "# \n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.schema.output_parser import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loader\n",
    "def data_loader(file_path= 'prompts/context.txt'):\n",
    "    loader = TextLoader(file_path)\n",
    "    documents = loader.load()\n",
    "\n",
    "    # Chunk the data\n",
    "    text_splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "    chunks = text_splitter.split_documents(documents)\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_retriever(chunks):\n",
    "\n",
    "  # Load OpenAI API key from .env file\n",
    "  load_dotenv(find_dotenv())\n",
    "\n",
    "  # Setup vector database\n",
    "  client = weaviate.Client(\n",
    "    embedded_options = EmbeddedOptions()\n",
    "  )\n",
    "\n",
    "  # Populate vector database\n",
    "  vectorstore = Weaviate.from_documents(\n",
    "      client = client,    \n",
    "      documents = chunks,\n",
    "      embedding = OpenAIEmbeddings(),\n",
    "      by_text = False\n",
    "  )\n",
    "\n",
    "  # Define vectorstore as retriever to enable semantic search\n",
    "  retriever = vectorstore.as_retriever()\n",
    "  return retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='OpenAI was initially founded in 2015 by Sam Altman, Elon Musk, Ilya Sutskever and Greg Brockman as a \\nnon-profit organization with the stated goal to “advance digital intelligence in the way that is most \\nlikely to benefit humanity as a whole.” The company assembled a team of the best researchers in the \\nfield of AI to pursue the goal of building AGI in a safe way.', metadata={'source': 'prompts/context.txt'}),\n",
       " Document(page_content='The early years of OpenAI were marked with rapid experimentation. The company made significant progress \\non research in deep learning and reinforcement learning, and released ‘OpenAI Gym’ in 2016, a toolkit \\nfor developing and comparing reinforcement learning algorithms.', metadata={'source': 'prompts/context.txt'}),\n",
       " Document(page_content='OpenAI showcased the capabilities of these reinforcement learning algorithms through its ‘OpenAI Five’ \\nproject in 2018, which trained five independent AI agents to play a complex multiplayer online battle \\narena game called ‘Dota 2’. Despite operating independently, these agents learned to work as a cohesive \\nteam to coordinate strategies within the game.', metadata={'source': 'prompts/context.txt'})]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started /Users/mahlettaye/.cache/weaviate-embedded: process ID 6103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{\"action\":\"startup\",\"default_vectorizer_module\":\"none\",\"level\":\"info\",\"msg\":\"the default vectorizer modules is set to \\\"none\\\", as a result all new schema classes without an explicit vectorizer setting, will use this vectorizer\",\"time\":\"2024-01-17T15:44:28+03:00\"}\n",
      "{\"action\":\"startup\",\"auto_schema_enabled\":true,\"level\":\"info\",\"msg\":\"auto schema enabled setting is set to \\\"true\\\"\",\"time\":\"2024-01-17T15:44:28+03:00\"}\n",
      "{\"level\":\"info\",\"msg\":\"No resource limits set, weaviate will use all available memory and CPU. To limit resources, set LIMIT_RESOURCES=true\",\"time\":\"2024-01-17T15:44:28+03:00\"}\n",
      "{\"level\":\"warning\",\"msg\":\"Multiple vector spaces are present, GraphQL Explore and REST API list objects endpoint module include params has been disabled as a result.\",\"time\":\"2024-01-17T15:44:28+03:00\"}\n",
      "{\"action\":\"grpc_startup\",\"level\":\"info\",\"msg\":\"grpc server listening at [::]:50060\",\"time\":\"2024-01-17T15:44:28+03:00\"}\n",
      "{\"action\":\"restapi_management\",\"level\":\"info\",\"msg\":\"Serving weaviate at http://127.0.0.1:8079\",\"time\":\"2024-01-17T15:44:29+03:00\"}\n",
      "/Users/mahlettaye/prompt-evaluation/venv/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.embeddings.openai.OpenAIEmbeddings` was deprecated in langchain-community 0.1.0 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAIEmbeddings`.\n",
      "  warn_deprecated(\n",
      "{\"level\":\"info\",\"msg\":\"Created shard langchain_c28e95b5f02d4a6d817409505d6046b4_LVNDQoo0jmdk in 4.293042ms\",\"time\":\"2024-01-17T15:44:29+03:00\"}\n",
      "{\"action\":\"hnsw_vector_cache_prefill\",\"count\":1000,\"index_id\":\"main\",\"level\":\"info\",\"limit\":1000000000000,\"msg\":\"prefilled vector cache\",\"time\":\"2024-01-17T15:44:29+03:00\",\"took\":526625}\n",
      "{\"level\":\"info\",\"msg\":\"Completed loading shard langchain_80fc6c5b037a45699de9c56ef2e277f1_acZEIIPfPh2y in 6.150833ms\",\"time\":\"2024-01-17T15:44:29+03:00\"}\n",
      "{\"level\":\"info\",\"msg\":\"Completed loading shard langchain_2b51683eea8e4494b7923fa5bff82da3_fyB9B9sAjtNZ in 6.527458ms\",\"time\":\"2024-01-17T15:44:29+03:00\"}\n",
      "{\"action\":\"hnsw_vector_cache_prefill\",\"count\":3000,\"index_id\":\"main\",\"level\":\"info\",\"limit\":1000000000000,\"msg\":\"prefilled vector cache\",\"time\":\"2024-01-17T15:44:29+03:00\",\"took\":399292}\n",
      "{\"action\":\"hnsw_vector_cache_prefill\",\"count\":3000,\"index_id\":\"main\",\"level\":\"info\",\"limit\":1000000000000,\"msg\":\"prefilled vector cache\",\"time\":\"2024-01-17T15:44:29+03:00\",\"took\":3097375}\n",
      "/Users/mahlettaye/prompt-evaluation/venv/lib/python3.10/site-packages/langchain_community/embeddings/openai.py:500: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.5/migration/\n",
      "  response = response.dict()\n",
      "/Users/mahlettaye/prompt-evaluation/venv/lib/python3.10/site-packages/pydantic/main.py:979: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.5/migration/\n",
      "  warnings.warn('The `dict` method is deprecated; use `model_dump` instead.', DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "chunks =  data_loader()\n",
    "retriever = create_retriever(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mahlettaye/prompt-evaluation/venv/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.chat_models.openai.ChatOpenAI` was deprecated in langchain-community 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Define LLM\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "# Define prompt template\n",
    "template = \"\"\"You are an assistant for question-answering tasks. \n",
    "Use the following pieces of retrieved context to answer the question. \n",
    "If you don't know the answer, just say that you don't know. \n",
    "Use two sentences maximum and keep the answer concise.\n",
    "Question: {question} \n",
    "Context: {context} \n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# Setup RAG pipeline\n",
    "rag_chain = (\n",
    "    {\"context\": retriever,  \"question\": RunnablePassthrough()} \n",
    "    | prompt \n",
    "    | llm\n",
    "    | StrOutputParser() \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mahlettaye/prompt-evaluation/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Users/mahlettaye/prompt-evaluation/venv/lib/python3.10/site-packages/langchain_community/embeddings/openai.py:500: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.5/migration/\n",
      "  response = response.dict()\n",
      "/Users/mahlettaye/prompt-evaluation/venv/lib/python3.10/site-packages/pydantic/main.py:979: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.5/migration/\n",
      "  warnings.warn('The `dict` method is deprecated; use `model_dump` instead.', DeprecationWarning)\n",
      "/Users/mahlettaye/prompt-evaluation/venv/lib/python3.10/site-packages/langchain_community/chat_models/openai.py:458: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.5/migration/\n",
      "  response = response.dict()\n",
      "/Users/mahlettaye/prompt-evaluation/venv/lib/python3.10/site-packages/pydantic/main.py:979: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.5/migration/\n",
      "  warnings.warn('The `dict` method is deprecated; use `model_dump` instead.', DeprecationWarning)\n",
      "/Users/mahlettaye/prompt-evaluation/venv/lib/python3.10/site-packages/langchain_community/embeddings/openai.py:500: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.5/migration/\n",
      "  response = response.dict()\n",
      "/Users/mahlettaye/prompt-evaluation/venv/lib/python3.10/site-packages/pydantic/main.py:979: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.5/migration/\n",
      "  warnings.warn('The `dict` method is deprecated; use `model_dump` instead.', DeprecationWarning)\n",
      "/Users/mahlettaye/prompt-evaluation/venv/lib/python3.10/site-packages/langchain_community/embeddings/openai.py:500: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.5/migration/\n",
      "  response = response.dict()\n",
      "/Users/mahlettaye/prompt-evaluation/venv/lib/python3.10/site-packages/pydantic/main.py:979: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.5/migration/\n",
      "  warnings.warn('The `dict` method is deprecated; use `model_dump` instead.', DeprecationWarning)\n",
      "/Users/mahlettaye/prompt-evaluation/venv/lib/python3.10/site-packages/langchain_community/chat_models/openai.py:458: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.5/migration/\n",
      "  response = response.dict()\n",
      "/Users/mahlettaye/prompt-evaluation/venv/lib/python3.10/site-packages/pydantic/main.py:979: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.5/migration/\n",
      "  warnings.warn('The `dict` method is deprecated; use `model_dump` instead.', DeprecationWarning)\n",
      "/Users/mahlettaye/prompt-evaluation/venv/lib/python3.10/site-packages/langchain_community/embeddings/openai.py:500: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.5/migration/\n",
      "  response = response.dict()\n",
      "/Users/mahlettaye/prompt-evaluation/venv/lib/python3.10/site-packages/pydantic/main.py:979: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.5/migration/\n",
      "  warnings.warn('The `dict` method is deprecated; use `model_dump` instead.', DeprecationWarning)\n",
      "/Users/mahlettaye/prompt-evaluation/venv/lib/python3.10/site-packages/langchain_community/embeddings/openai.py:500: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.5/migration/\n",
      "  response = response.dict()\n",
      "/Users/mahlettaye/prompt-evaluation/venv/lib/python3.10/site-packages/pydantic/main.py:979: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.5/migration/\n",
      "  warnings.warn('The `dict` method is deprecated; use `model_dump` instead.', DeprecationWarning)\n",
      "/Users/mahlettaye/prompt-evaluation/venv/lib/python3.10/site-packages/langchain_community/chat_models/openai.py:458: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.5/migration/\n",
      "  response = response.dict()\n",
      "/Users/mahlettaye/prompt-evaluation/venv/lib/python3.10/site-packages/pydantic/main.py:979: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.5/migration/\n",
      "  warnings.warn('The `dict` method is deprecated; use `model_dump` instead.', DeprecationWarning)\n",
      "/Users/mahlettaye/prompt-evaluation/venv/lib/python3.10/site-packages/langchain_community/embeddings/openai.py:500: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.5/migration/\n",
      "  response = response.dict()\n",
      "/Users/mahlettaye/prompt-evaluation/venv/lib/python3.10/site-packages/pydantic/main.py:979: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.5/migration/\n",
      "  warnings.warn('The `dict` method is deprecated; use `model_dump` instead.', DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "questions = [\"Who founded OpenAI?\", \n",
    "             \"What was the initial goal of OpenAI?\",\n",
    "             \"What did OpenAI release in 2016?\",\n",
    "            ]\n",
    "ground_truths = [[\"Sam Altman, Elon Musk, Ilya Sutskever and Greg Brockman\"],\n",
    "                [\"To advance digital intelligence in a way that benefits humanity\"],\n",
    "                [\"OpenAI Gym, a toolkit for developing and comparing reinforcement learning algorithms\"]]\n",
    "answers = []\n",
    "contexts = []\n",
    "\n",
    "# Inference\n",
    "for query in questions:\n",
    "\n",
    "  answers.append(rag_chain.invoke(query))\n",
    "  contexts.append([docs.page_content for docs in retriever.get_relevant_documents(query)])\n",
    "\n",
    "# To dict\n",
    "data = {\n",
    "    \"question\": questions, # list \n",
    "    \"answer\": answers, # list\n",
    "    \"contexts\": contexts, # list list\n",
    "    \"ground_truths\": ground_truths # list Lists\n",
    "}\n",
    "\n",
    "# Convert dict to dataset\n",
    "dataset = Dataset.from_dict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating with [context_precision]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:06<00:00,  6.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating with [context_recall]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:10<00:00, 10.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating with [faithfulness]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:02<00:00,  2.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating with [answer_relevancy]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]/Users/mahlettaye/prompt-evaluation/venv/lib/python3.10/site-packages/langchain_community/embeddings/openai.py:500: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.5/migration/\n",
      "  response = response.dict()\n",
      "/Users/mahlettaye/prompt-evaluation/venv/lib/python3.10/site-packages/pydantic/main.py:979: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.5/migration/\n",
      "  warnings.warn('The `dict` method is deprecated; use `model_dump` instead.', DeprecationWarning)\n",
      "100%|██████████| 1/1 [00:04<00:00,  4.50s/it]\n"
     ]
    }
   ],
   "source": [
    "from ragas import evaluate\n",
    "from ragas.metrics import (\n",
    "    faithfulness,\n",
    "    answer_relevancy,\n",
    "    context_recall,\n",
    "    context_precision,\n",
    ")\n",
    "\n",
    "result = evaluate(\n",
    "    dataset = dataset, \n",
    "    metrics=[\n",
    "        context_precision,\n",
    "        context_recall,\n",
    "        faithfulness,\n",
    "        answer_relevancy,\n",
    "    ],\n",
    ")\n",
    "\n",
    "df = result.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>contexts</th>\n",
       "      <th>ground_truths</th>\n",
       "      <th>context_precision</th>\n",
       "      <th>context_recall</th>\n",
       "      <th>faithfulness</th>\n",
       "      <th>answer_relevancy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Who founded OpenAI?</td>\n",
       "      <td>OpenAI was founded by Sam Altman, Elon Musk, I...</td>\n",
       "      <td>[OpenAI was initially founded in 2015 by Sam A...</td>\n",
       "      <td>[Sam Altman, Elon Musk, Ilya Sutskever and Gre...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.959200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What was the initial goal of OpenAI?</td>\n",
       "      <td>The initial goal of OpenAI was to advance digi...</td>\n",
       "      <td>[OpenAI was initially founded in 2015 by Sam A...</td>\n",
       "      <td>[To advance digital intelligence in a way that...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What did OpenAI release in 2016?</td>\n",
       "      <td>OpenAI released 'OpenAI Gym' in 2016, a toolki...</td>\n",
       "      <td>[The early years of OpenAI were marked with ra...</td>\n",
       "      <td>[OpenAI Gym, a toolkit for developing and comp...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.899171</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               question  \\\n",
       "0                   Who founded OpenAI?   \n",
       "1  What was the initial goal of OpenAI?   \n",
       "2      What did OpenAI release in 2016?   \n",
       "\n",
       "                                              answer  \\\n",
       "0  OpenAI was founded by Sam Altman, Elon Musk, I...   \n",
       "1  The initial goal of OpenAI was to advance digi...   \n",
       "2  OpenAI released 'OpenAI Gym' in 2016, a toolki...   \n",
       "\n",
       "                                            contexts  \\\n",
       "0  [OpenAI was initially founded in 2015 by Sam A...   \n",
       "1  [OpenAI was initially founded in 2015 by Sam A...   \n",
       "2  [The early years of OpenAI were marked with ra...   \n",
       "\n",
       "                                       ground_truths  context_precision  \\\n",
       "0  [Sam Altman, Elon Musk, Ilya Sutskever and Gre...                1.0   \n",
       "1  [To advance digital intelligence in a way that...                1.0   \n",
       "2  [OpenAI Gym, a toolkit for developing and comp...                1.0   \n",
       "\n",
       "   context_recall  faithfulness  answer_relevancy  \n",
       "0             1.0           1.0          0.959200  \n",
       "1             1.0           1.0          1.000000  \n",
       "2             1.0           1.0          0.899171  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Integration with Retrieval-Augmented Generation Assessment:\n",
    "##### Monte Carlo for Robustness Testing: Use Monte Carlo simulations to test the robustness of the RAG system across a wide range of possible retrieval scenarios. This helps in understanding how different types of retrieved information can impact the quality of the generated content.\n",
    "##### Elo Rating for Continuous Improvement: Utilize the Elo rating system to continuously assess and improve the RAG model. By comparing new outputs with previous ones and adjusting ratings accordingly, the system can learn which types of retrieval-augmented generations work best."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7f3c0b24c41e80e94b84b01069c679ceb9a0604be81ab76bb66c9ea948f7d76b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
